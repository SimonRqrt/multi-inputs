{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(0)\n",
        "rn.seed(1)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=0'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ7B_1OLSse7",
        "outputId": "83d9e0f7-72d4-4265-f4ea-eb8878f194cb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "U_mZah7rSkTr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"category_name\"] = df[\"product_category_tree\"].str.split(r\">>\", regex=False, expand=True)[0]\n",
        "df[\"category_name\"] = df[\"category_name\"].str.replace('[\"', '')"
      ],
      "metadata": {
        "id": "VUdVbCVMagZK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category_name'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "rsePnidSazOF",
        "outputId": "88c9fa03-c2f2-4781-8316-bcbcc783ed37"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category_name\n",
              "Home Furnishing                150\n",
              "Baby Care                      150\n",
              "Watches                        150\n",
              "Home Decor & Festive Needs     150\n",
              "Kitchen & Dining               150\n",
              "Beauty and Personal Care       150\n",
              "Computers                      150\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Home Furnishing</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Baby Care</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Watches</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Home Decor &amp; Festive Needs</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kitchen &amp; Dining</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beauty and Personal Care</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Computers</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_list = {'Home Furnishing' : 0 ,\n",
        "                 'Baby Care' : 1 ,\n",
        "                 'Watches' : 2 ,\n",
        "                 'Home Decor & Festive Needs' : 3 ,\n",
        "                 'Kitchen & Dining' : 4\t,\n",
        "                 'Beauty and Personal Care' : 5 ,\n",
        "                 'Computers' : 6}\n",
        "\n",
        "df['category_name'].replace(category_list, inplace=True)"
      ],
      "metadata": {
        "id": "Wuy7JlgqBlya",
        "outputId": "7cd7d115-162c-45b0-fdd4-27935c29f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-7f625f2c4597>:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['category_name'].replace(category_list, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Lambda, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "bert_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "for layer in bert_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "max_sequence_length = 512\n",
        "\n",
        "X = np.array(df['description'])\n",
        "y = df['category_name']\n",
        "\n",
        "# Division des données\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y, shuffle=True)\n",
        "\n",
        "inputs_text = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"inputs_text\")\n",
        "\n",
        "def extract_token(tensor):\n",
        "    outputs_text = bert_model(tensor)\n",
        "    token = outputs_text.last_hidden_state[:, 0, :]\n",
        "    return token\n",
        "\n",
        "bert_model_output = Lambda(extract_token, output_shape=(768,))(inputs_text)\n",
        "\n",
        "text_output = Dense(128, activation='relu', name=\"text_output\")(bert_model_output)\n",
        "\n",
        "\n",
        "img_input_shape = (224, 224, 3)\n",
        "img_input = Input(shape=img_input_shape, name=\"img_input\")\n",
        "\n",
        "cv_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=img_input_shape)\n",
        "\n",
        "cv_model.trainable = False\n",
        "\n",
        "x = cv_model(img_input, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "cv_output = Dense(128, activation='relu', name=\"cv_output\")(x)\n",
        "\n",
        "# Fusion des sorties\n",
        "fusion = concatenate([text_output, cv_output])\n",
        "\n",
        "# Couche de sortie\n",
        "output_layer = Dense(7, activation='softmax')(fusion)\n",
        "\n",
        "# Définition du modèle final\n",
        "model_final = Model(\n",
        "    inputs=[inputs_text, img_input],\n",
        "    outputs=output_layer\n",
        ")\n",
        "\n",
        "# Compilation du modèle\n",
        "model_final.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j06K3nJ5bOfQ",
        "outputId": "adea637e-f285-4535-ca85-786792af0f7f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_final.summary()"
      ],
      "metadata": {
        "id": "um4bR3jNHrAn",
        "outputId": "cf3eb5eb-4250-4cdf-d020-ebb7966eb3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ img_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ inputs_text (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ MobileNetV3Small          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │        \u001b[38;5;34m939,120\u001b[0m │ img_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_9 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ inputs_text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ MobileNetV3Small[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_output (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m98,432\u001b[0m │ lambda_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cv_output (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m73,856\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ text_output[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ cv_output[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │          \u001b[38;5;34m1,799\u001b[0m │ concatenate_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ img_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ inputs_text (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ MobileNetV3Small          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │ img_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ inputs_text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ MobileNetV3Small[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ lambda_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cv_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ cv_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │ concatenate_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,113,207\u001b[0m (4.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,113,207</span> (4.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m174,087\u001b[0m (680.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,087</span> (680.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"serves as both a practical necessity and a fashion accessory\"\n",
        "\n",
        "encoding = tokenizer(example_text, return_tensors='tf', padding=\"max_length\", truncation=True, max_length=max_sequence_length)\n",
        "input_ids = encoding['input_ids']\n",
        "attention_mask = encoding['attention_mask']\n",
        "\n",
        "def text_encoding(text, tokenizer, max_sequence_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for t in text:\n",
        "        encoding = tokenizer(t, return_tensors='tf', padding=\"max_length\", truncation=True, max_length=max_sequence_length)\n",
        "        input_ids.append(encoding['input_ids'][0])\n",
        "    return np.array(input_ids)\n",
        "\n",
        "X_train_text = text_encoding(X_train_text, tokenizer, max_sequence_length)\n",
        "X_test_text = text_encoding(X_test_text, tokenizer, max_sequence_length)\n",
        "\n",
        "X_train_text = tf.convert_to_tensor(X_train_text, dtype=tf.int32)\n",
        "X_test_text = tf.convert_to_tensor(X_test_text, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "n3mlUYSHKoz7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.mobilenet_v3 import preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "img_path = '/content/drive/MyDrive/Flipkart/Images/'\n",
        "image_filenames = list(df.image)\n",
        "\n",
        "# Split the filenames and labels first\n",
        "X_train_filenames, X_test_filenames, y_train, y_test = train_test_split(\n",
        "    image_filenames, y, test_size=0.2, random_state=0, stratify=y, shuffle=True\n",
        ")\n",
        "\n",
        "def load_and_preprocess_image(path=img_path, list_img = image_filenames):\n",
        "    img_array = []\n",
        "    for img in list_img:\n",
        "      image = load_img(path + '/' + img, target_size=(224, 224), interpolation='nearest')\n",
        "      array = img_to_array(image)\n",
        "      array = preprocess_input(array)\n",
        "      img_array.append(array)\n",
        "    return np.array(img_array)\n",
        "\n",
        "data_img = load_and_preprocess_image()\n",
        "array_img = data_img[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "l5XK6wNQIA7p",
        "outputId": "461f9f50-4fc2-470a-aded-2a56af0143af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess images for training and testing sets separately\n",
        "X_train_img = load_and_preprocess_image(path=img_path, list_img=X_train_filenames)\n",
        "X_test_img = load_and_preprocess_image(path=img_path, list_img=X_test_filenames)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on your training labels\n",
        "label_encoder.fit(y_train_text)\n",
        "\n",
        "# Transform the training and testing labels\n",
        "y_train_encoded = label_encoder.transform(y_train_text)\n",
        "y_test_encoded = label_encoder.transform(y_test_text)\n",
        "\n",
        "# Convert the encoded labels to TensorFlow tensors\n",
        "y_train_text = tf.convert_to_tensor(y_train_encoded, dtype=tf.int32)\n",
        "y_test_text = tf.convert_to_tensor(y_test_encoded, dtype=tf.int32)\n",
        "\n",
        "\n",
        "history = model_final.fit(\n",
        "    [X_train_text, X_train_img],\n",
        "    y_train_text,\n",
        "    batch_size=32,\n",
        "    verbose=\"auto\",\n",
        "    validation_split=0.2,\n",
        "    shuffle=True,\n",
        "    class_weight=None,\n",
        "    sample_weight=None,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    validation_batch_size=32,\n",
        "    validation_freq=1,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "3UmuEUrHMbb3",
        "outputId": "bcdc020e-cd09-4ef5-f0e3-c9e30080b494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.1387 - loss: 2.1018 - val_accuracy: 0.3750 - val_loss: 1.7378\n",
            "Epoch 2/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 751ms/step - accuracy: 0.4215 - loss: 1.6591 - val_accuracy: 0.5417 - val_loss: 1.4395\n",
            "Epoch 3/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 650ms/step - accuracy: 0.6137 - loss: 1.3661 - val_accuracy: 0.6845 - val_loss: 1.2170\n",
            "Epoch 4/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 637ms/step - accuracy: 0.6991 - loss: 1.1503 - val_accuracy: 0.7202 - val_loss: 1.0540\n",
            "Epoch 5/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 758ms/step - accuracy: 0.7459 - loss: 0.9890 - val_accuracy: 0.7440 - val_loss: 0.9365\n",
            "Epoch 6/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 749ms/step - accuracy: 0.7978 - loss: 0.8661 - val_accuracy: 0.7560 - val_loss: 0.8510\n",
            "Epoch 7/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 624ms/step - accuracy: 0.8129 - loss: 0.7719 - val_accuracy: 0.7738 - val_loss: 0.7894\n",
            "Epoch 8/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 762ms/step - accuracy: 0.8229 - loss: 0.6992 - val_accuracy: 0.7738 - val_loss: 0.7438\n",
            "Epoch 9/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 750ms/step - accuracy: 0.8346 - loss: 0.6420 - val_accuracy: 0.7917 - val_loss: 0.7094\n",
            "Epoch 10/10\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 638ms/step - accuracy: 0.8355 - loss: 0.5952 - val_accuracy: 0.7976 - val_loss: 0.6827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_final.evaluate([X_test_text, X_test_img], y_test_text)"
      ],
      "metadata": {
        "id": "ebbi6dCWPFQI",
        "outputId": "f4ccce2a-6a78-4b4d-fe6d-974455bc8481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8140 - loss: 0.7068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7577314972877502, 0.776190459728241]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "///////////////////////////////////////////////////////////////////\n",
        "///////////////////////////////////////////////////////////////////\n",
        "\n",
        "**IMAGE CLASSIFICATION**  (Ne pas utiliser à partir d'ici, résultats médiocres voir non fonctionnels)\n",
        "\n",
        "///////////////////////////////////////////////////////////////////\n",
        "///////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "xGTHSgf_hE8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Charger les données depuis le CSV\n",
        "data_path = '/content/drive/MyDrive/Flipkart/flipkart_com-ecommerce_sample_1050.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Extraire les chemins d'images et les catégories\n",
        "data['image_path'] = '/content/drive/MyDrive/Flipkart/Images/' + data['image']\n",
        "data['category'] = data['product_category_tree'].apply(lambda x: eval(x)[0].split(\" >> \")[0])\n",
        "data[\"category\"] = data[\"category\"].str.replace('[\"', '')\n",
        "\n",
        "# Paramètres de l'image et du modèle\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 16\n",
        "\n",
        "# Préparation des données avec des augmentations d'images\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Générateur pour l'entraînement et la validation\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    data,\n",
        "    x_col='image_path',\n",
        "    y_col='category',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    data,\n",
        "    x_col='image_path',\n",
        "    y_col='category',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Charger MobileNetV3\n",
        "base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "base_model.trainable = False  # Garder les couches de MobileNetV3 gelées pour l'apprentissage par transfert\n",
        "\n",
        "# Construire le modèle\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "EluYT-AyhIKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des poids de classe\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Callback pour le taux d'apprentissage\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[lr_reduction]  # Ajouter le callback\n",
        ")\n"
      ],
      "metadata": {
        "id": "hEkwwTY5x0dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tracer la précision\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Précision d\\'entraînement')\n",
        "plt.plot(history.history['val_accuracy'], label='Précision de validation')\n",
        "plt.title('Précision du modèle')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Précision')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Tracer la perte\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Perte d\\'entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Perte de validation')\n",
        "plt.title('Perte du modèle')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jAUkoDllWwfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FnUPzILBj-BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Concatenate, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Fusion des sorties image et texte\n",
        "combined = Concatenate()([image_output, text_output])\n",
        "combined_output = Dense(64, activation='relu')(combined)\n",
        "combined_output = Dropout(0.3)(combined_output)\n",
        "final_output = Dense(len(train_generator.class_indices), activation='softmax')(combined_output)\n",
        "\n",
        "# Modèle multi-input final\n",
        "final_model = Model(inputs=[resnet_base.input, text_input], outputs=final_output)\n",
        "\n",
        "# Compilation du modèle\n",
        "final_model.compile(optimizer=SGD(learning_rate=1e-4, momentum=0.9),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle avec les générateurs et les données textuelles\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "final_model.fit(\n",
        "    [train_generator, text_features],\n",
        "    validation_data=(validation_generator, X_test_text),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    class_weight=class_weights\n",
        ")"
      ],
      "metadata": {
        "id": "Yg5BXnarpojx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi inputs, combinaison des deux modèles**"
      ],
      "metadata": {
        "id": "MkFB84eEzQxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Création du modèle combiné\n",
        "text_model_output = tf.keras.layers.Input(shape=(768,), name='text_input')  # La taille du vecteur de DistilBERT est 768\n",
        "image_model_output = tf.keras.layers.Input(shape=(img_height[0], img_width[1], 3), name='image_input')\n",
        "\n",
        "# Passer l'entrée d'image à EfficientNet\n",
        "base_output = base_model(image_model_output)\n",
        "\n",
        "# Aplatir la sortie d'EfficientNet\n",
        "base_output_flat = layers.Flatten()(base_output)\n",
        "\n",
        "# Fusion des sorties\n",
        "x_combined = layers.Concatenate()([text_model_output, base_output_flat])\n",
        "\n",
        "# Ajout de couches supplémentaires pour le modèle de fusion\n",
        "x_combined = layers.Dense(128, activation='relu')(x_combined)\n",
        "x_combined = layers.Dropout(0.2)(x_combined)\n",
        "final_output = layers.Dense(train_dataset.cardinality().numpy(), activation='softmax')(x_combined)  # Nombre de classes\n",
        "\n",
        "# Construction du modèle combiné\n",
        "combined_model = Model(inputs=[text_model_output, image_model_output], outputs=final_output)\n",
        "\n",
        "# Compilation du modèle combiné\n",
        "combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Préparer les données d'entraînement et de validation en tant que tableaux NumPy\n",
        "X_image_train = np.concatenate([x.numpy() for x, _ in train_dataset])\n",
        "y_image_train = np.concatenate([y.numpy() for _, y in train_dataset])\n",
        "\n",
        "X_image_val = np.concatenate([x.numpy() for x, _ in validation_dataset])\n",
        "y_image_val = np.concatenate([y.numpy() for _, y in validation_dataset])\n",
        "\n",
        "# Encoder les étiquettes\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Entraîner le modèle combiné\n",
        "combined_model.fit(\n",
        "    [X_train, X_image_train],  # Entrées : vecteurs de texte et tableaux d'images\n",
        "    y_train_encoded,  # Utiliser les étiquettes encodées\n",
        "    validation_data=([X_test, X_image_val], y_test_encoded),  # Validation avec les données de validation\n",
        "    epochs=10  # Vous pouvez ajuster le nombre d'époques\n",
        ")\n"
      ],
      "metadata": {
        "id": "l-vNPBEB_5gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du modèle sur l'ensemble de test\n",
        "test_loss, test_accuracy = combined_model.evaluate([X_test, X_image_val], y_test_encoded)\n",
        "\n",
        "print(f'Perte sur l\\'ensemble de test : {test_loss:.4f}')\n",
        "print(f'Précision sur l\\'ensemble de test : {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "A0ysRSNYXlnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Prédire les étiquettes pour l'ensemble de test\n",
        "y_pred = combined_model.predict([X_test, X_image_val])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Créer une matrice de confusion\n",
        "cm = confusion_matrix(y_test_encoded, y_pred_classes)\n",
        "\n",
        "# Afficher la matrice de confusion\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Matrice de confusion')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JPTiAt6AX4L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Afficher le rapport de classification\n",
        "report = classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "JDBI0bW2YGPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}