{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ7B_1OLSse7",
        "outputId": "24558faf-ff89-4280-baf7-30d4dbe835d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "U_mZah7rSkTr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"category_name\"] = df[\"product_category_tree\"].str.split(r\">>\", regex=False, expand=True)[0]\n",
        "df[\"category_name\"] = df[\"category_name\"].str.replace('[\"', '')"
      ],
      "metadata": {
        "id": "VUdVbCVMagZK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category_name'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "rsePnidSazOF",
        "outputId": "b49a5f0c-effd-49b2-8051-e9d32d26a9c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category_name\n",
              "Home Furnishing                150\n",
              "Baby Care                      150\n",
              "Watches                        150\n",
              "Home Decor & Festive Needs     150\n",
              "Kitchen & Dining               150\n",
              "Beauty and Personal Care       150\n",
              "Computers                      150\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Home Furnishing</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Baby Care</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Watches</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Home Decor &amp; Festive Needs</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kitchen &amp; Dining</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beauty and Personal Care</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Computers</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************************\n",
        "\n",
        "**Ajout des images dans des sous dossiers spécifiques à leur catégorie**\n",
        "\n",
        "*****************************************************\n"
      ],
      "metadata": {
        "id": "bl4cu_jrmzui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Chemin vers le répertoire contenant les images\n",
        "base_dir = '/content/drive/MyDrive/Flipkart/Images'\n",
        "\n",
        "# Créer les sous-dossiers de catégories s'ils n'existent pas\n",
        "for category in df['category_name'].unique():\n",
        "    os.makedirs(os.path.join(base_dir, category), exist_ok=True)\n",
        "\n",
        "# Déplacer chaque image dans le dossier de sa catégorie\n",
        "for _, row in df.iterrows():\n",
        "    src_path = os.path.join(base_dir, row['image'])  # chemin d'origine de l'image\n",
        "    dest_path = os.path.join(base_dir, row['category_name'], row['image'])  # chemin de destination\n",
        "    if os.path.exists(src_path):  # vérifier si le fichier existe pour éviter les erreurs\n",
        "        shutil.move(src_path, dest_path)\n",
        "    else:\n",
        "        print(f\"Image non trouvée : {src_path}\")\n",
        "\n",
        "print(\"Images regroupées par catégorie.\")\n"
      ],
      "metadata": {
        "id": "KG7B3OZqkZbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "descriptions = df[\"description\"].tolist()\n",
        "categories = df[\"category_name\"].tolist()\n",
        "\n",
        "# Division des données\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    descriptions, categories, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "R0sMfp0va9XD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts, padding=True, truncation=True, return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_texts(train_texts)\n",
        "val_encodings = tokenize_texts(val_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j06K3nJ5bOfQ",
        "outputId": "1376006f-8067-4456-d24e-4371f889edbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "embeddings = model(input_ids, attention_mask=attention_mask)[0]\n",
        "output = tf.keras.layers.GlobalAveragePooling1D()(embeddings)\n",
        "output = tf.keras.layers.Dense(len(set(categories)), activation=\"softmax\")(output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "val_labels = tf.convert_to_tensor(val_labels)\n"
      ],
      "metadata": {
        "id": "A5VnBMZBcHWL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encodage des catégories\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "\n",
        "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
        "val_labels = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n",
        "\n",
        "model.fit(\n",
        "    {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
        "    train_labels,\n",
        "    validation_data=(\n",
        "        {\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"]},\n",
        "        val_labels\n",
        "    ),\n",
        "    epochs=3,\n",
        "    batch_size=16\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1KP_1jiOchl9",
        "outputId": "e2e38804-072d-41a5-cb42-13409b4d36ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall_101 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-9-ee8812c91d6a>\", line 11, in <cell line: 11>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 621, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1300, in apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 729, in apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1330, in _internal_apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1422, in _distributed_apply_gradients_fn\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1417, in apply_grad_to_update_var\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node Adam/StatefulPartitionedCall_101}}]] [Op:__inference_train_function_17781]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ee8812c91d6a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node Adam/StatefulPartitionedCall_101 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-9-ee8812c91d6a>\", line 11, in <cell line: 11>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 621, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1300, in apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 729, in apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1330, in _internal_apply_gradients\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1422, in _distributed_apply_gradients_fn\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1417, in apply_grad_to_update_var\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node Adam/StatefulPartitionedCall_101}}]] [Op:__inference_train_function_17781]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMAGE CLASSIFICATION**"
      ],
      "metadata": {
        "id": "xGTHSgf_hE8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)  # Pour EfficientNet, utiliser (299, 299) pour InceptionV3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Chargement des images depuis le répertoire avec partition en ensembles d’entraînement et de validation\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Flipkart/Images',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Flipkart/Images',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "EluYT-AyhIKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Normalisation pour correspondre à l'entraînement ImageNet\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(data_augmentation(x)), y))\n",
        "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "metadata": {
        "id": "HUbKf6hOld3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0  # Remplacer par InceptionV3 si nécessaire\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Chargement du modèle pré-entraîné sans la dernière couche de classification\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "# Congélation des poids du modèle de base pour un fine-tuning minimal\n",
        "base_model.trainable = False\n",
        "\n",
        "# Ajout des nouvelles couches\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dropout(0.2)(x)  # Regularisation pour éviter l'overfitting\n",
        "output = layers.Dense(train_generator.num_classes, activation='softmax')(x)  # Nombre de classes\n",
        "\n",
        "# Construction du modèle\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "0chJHnXtlnDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}